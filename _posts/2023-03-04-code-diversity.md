---
layout: post
title: How diverse are the generations of CodeLMs ?
date: 2023-03-04 17:00:00
tags: codelms analysis benchmarking lm
description: Observing, Measuring the diversity of CodeLM Generations.
mermaid:
  enabled: true
  zoomable: true
---
## Introduction
Code Language Models are used in the context of code completion and chat interfaces day in day out by software developers to write and contribute to code bases. Programming Languages is a formal language with a fixed, strict set of rules.  This pushes us to quanitify and measure how diverse are the generations generated by these models in various settings. It is also crucial to understand and define diversity in the context of code LMs. This post aims to explore and understand the diversity of generations of Code LMs. And discover various shortcomings of diversity of generations of Code LMs in a controlled setting. In the below post I use [HumanEval benchmark](https://github.com/openai/human-eval) which is a benchmark evaluating function level code completion by CodeLMs for functional correctness measuring the accuracy, passing the given test cases within first `k` out of `n` samples. 
## Semantic and Syntactic Diversity
The general notion of syntax and semantics in the context of programming languages is slightly different from that of natural languages. In the context of programming languages syntax refers to the structure of the code, the rules that govern the structure of the code. Semantics refers to the intention of a given piece of code. Here is a small example of semantically similar code snippets which can be expressed varied syntax.

{% include figure.liquid loading="eager" path="assets/img/semantic_similar.png" class="semantically similar code" zoomable=true %}

 The formal structure of the code allows for a wide range of syntactic variations. This is a key difference between natural languages and programming languages. 

## HumanEval Benchmark
[HumanEval benchmark](https://github.com/openai/human-eval) introduced in the [Codex](https://arxiv.org/abs/2107.03374) is a popular benchmark widely used to measure the performance of CodeLMs. It is a benchmark evaluating function level code completion by CodeLMs for functional correctness measuring the accuracy, passing the given test cases within first `k` out of `n` samples. 

## Diversity Metrics
The most basic coherent way to measure diversity would be to look at the latent representations of different completions given a prompt. This can be done by using a good embedding model and doing some cluster analysis in the latent representation of different completion given.

{% include figure.liquid loading="eager" path="assets/img/div_pipeline.png" class="diversity codeLMs" zoomable=true %}



## Cite
```
@inproceedings{reshint-code-lm-div,
    title = "How diverse are the generations of CodeLMs ?",
    author = "Reshinth Adithyan",
    month = april,
    year = "2024"}
```
