<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://reshinthadithyan.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://reshinthadithyan.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-04-04T22:53:30+00:00</updated><id>https://reshinthadithyan.github.io/feed.xml</id><title type="html">Reshinth Adithyan</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Attempts to measure diversity of CodeLM Generations ? [WIP]</title><link href="https://reshinthadithyan.github.io/blog/2023/code-diversity/" rel="alternate" type="text/html" title="Attempts to measure diversity of CodeLM Generations ? [WIP]"/><published>2023-03-04T17:00:00+00:00</published><updated>2023-03-04T17:00:00+00:00</updated><id>https://reshinthadithyan.github.io/blog/2023/code-diversity</id><content type="html" xml:base="https://reshinthadithyan.github.io/blog/2023/code-diversity/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>Code Language Models are used in the context of code completion and chat interfaces day in day out by software developers to write and contribute to code bases. Programming Languages is a formal language with a fixed, strict set of rules. This pushes us to quanitify and measure how diverse are the generations generated by these models in various settings. It is also crucial to understand and define diversity in the context of code LMs. This post aims to explore and understand the diversity of generations of Code LMs. And discover various shortcomings of diversity of generations of Code LMs in a controlled setting. In the below post I use <a href="https://github.com/openai/human-eval">HumanEval benchmark</a> which is a benchmark evaluating function level code completion by CodeLMs for functional correctness measuring the accuracy, passing the given test cases within first <code class="language-plaintext highlighter-rouge">k</code> out of <code class="language-plaintext highlighter-rouge">n</code> samples.</p> <h2 id="semantic-and-syntactic-diversity">Semantic and Syntactic Diversity</h2> <p>The general notion of syntax and semantics in the context of programming languages is slightly different from that of natural languages. In the context of programming languages syntax refers to the structure of the code, the rules that govern the structure of the code. Semantics refers to the intention of a given piece of code. Here is a small example of semantically similar code snippets which can be expressed varied syntax.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/semantic_similar-480.webp 480w,/assets/img/semantic_similar-800.webp 800w,/assets/img/semantic_similar-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/semantic_similar.png" class="semantically similar code" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>The formal structure of the code allows for a wide range of syntactic variations. This is a key difference between natural languages and programming languages.</p> <h2 id="humaneval-benchmark">HumanEval Benchmark</h2> <p><a href="https://github.com/openai/human-eval">HumanEval benchmark</a> introduced in the <a href="https://arxiv.org/abs/2107.03374">Codex</a> is a popular benchmark widely used to measure the performance of CodeLMs. It is a benchmark evaluating function level code completion by CodeLMs for functional correctness measuring the accuracy, passing the given test cases within first <code class="language-plaintext highlighter-rouge">k</code> out of <code class="language-plaintext highlighter-rouge">n</code> samples.</p> <h2 id="diversity-metrics">Diversity Metrics</h2> <p>The coherent way to quantify diversity would be to look at the latent representations of different completions given a prompt. This can be done by using a good embedding model and doing some cluster analysis in the latent representation of different completion given. While this method has itâ€™s shortcomings, it is a good starting point to quantify diversity. But the ideal end goal would be to measure how semantically diverse and functionally equivalent the generations are given a singular prompt.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/div_pipeline-480.webp 480w,/assets/img/div_pipeline-800.webp 800w,/assets/img/div_pipeline-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/div_pipeline.png" class="diversity codeLMs" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="limitation-of-code-embedding-models-to-model-semantics">Limitation of Code Embedding models to model semantics</h2> <p>Unlike natural language wherein, the semantics of a sentence can be understood by looking at the words and their order, in the context of programming languages, the semantics of a code snippet is not just the syntactical elements and their order. The semantics of a code snippet is also dependent on the structure of the code, the states used, the functions called, the libraries imported, etc. This makes it difficult to model the semantics of a code snippet using just the lexical tokens and their order. And this makes the model heavily biased to syntax rather than actual semantic similarity. For instance, consider three functions,</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">func_a</span><span class="p">(</span><span class="n">l</span><span class="p">):</span>
    <span class="k">return</span> <span class="nf">max</span><span class="p">(</span><span class="n">l</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">func_b</span><span class="p">(</span><span class="n">l</span><span class="p">):</span>
  <span class="n">max_element</span> <span class="o">=</span> <span class="n">l</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">l</span><span class="p">:</span>
      <span class="k">if</span> <span class="n">i</span> <span class="o">&gt;</span> <span class="n">max_element</span><span class="p">:</span>
          <span class="n">max_element</span> <span class="o">=</span> <span class="n">i</span>
  <span class="k">return</span> <span class="n">max_element</span>

<span class="k">def</span> <span class="nf">func_c</span><span class="p">(</span><span class="n">l</span><span class="p">):</span>
  <span class="n">max_element</span> <span class="o">=</span> <span class="n">l</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> 
  <span class="k">return</span> <span class="n">max_element</span>
</code></pre></div></div> <p><code class="language-plaintext highlighter-rouge">func_a</code> and <code class="language-plaintext highlighter-rouge">func_b</code> are semantically similar functionally trying to find the maximum element in a list. But <code class="language-plaintext highlighter-rouge">func_c</code> is actually bugged though naming conventions of the variable tends to be make the model map the latents to be similar. Though the actual semantically similar functional equivalent of <code class="language-plaintext highlighter-rouge">func_a</code> is <code class="language-plaintext highlighter-rouge">func_b</code>. The model tend to be severely biased towards the syntax rather than the actual semantics of the code.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/embd_plot-480.webp 480w,/assets/img/embd_plot-800.webp 800w,/assets/img/embd_plot-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/embd_plot.png" class="Embedding dataset" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="observations">Observations</h2> <p>For the sake of controlled experiment, we take the following models and do the analysis on the following models,</p> <ul> <li>deepseek-coder-1.3B</li> <li>deepseek-coder-6.7B</li> <li>deepseek-coder-33B <br/> The choice is with the assumption that they are trained on similar if not same amount of data and are base models with no high quality fine-tuning data encapsulated in the training run, since the dynamics of generation quality might differ post finetuning phase.</li> </ul> <div> <iframe width="711" height="229" seamless="" frameborder="0" scrolling="no" src="https://docs.google.com/spreadsheets/d/e/2PACX-1vS52RmWqYdfJTxe_gfQScU3eGNIdGpZ-Zs2LqQ6rMq4tOVBiPmONN6Dn-BiC7ks3Sam6B4B7Y-XN9DP/pubchart?oid=598483660&amp;format=interactive"></iframe> </div> <p>You can explore through the solutions generated by these models <a href="https://huggingface.co/spaces/reshinthadith/CodeGen-Diversity">here</a>.</p> <h2 id="conclusion-and-future-directions">Conclusion and Future Directions</h2> <p>Proper measure of diversity defined as <code class="language-plaintext highlighter-rouge">functionally equivalent, semantically similar</code> can act as a secondary metric to count onto in addition to performance of a model in correctness of generating solution for the given problem. When a model generates high quality diverse generations, that increases the probability of the output search space containing the solution of solving a problem in a given turn.</p> <h2 id="cite">Cite</h2> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@inproceedings{reshint-code-lm-div,
    title = "How diverse are the generations of CodeLMs ?",
    author = "Reshinth Adithyan",
    month = april,
    year = "2024"}
</code></pre></div></div>]]></content><author><name></name></author><category term="codelms"/><category term="analysis"/><category term="benchmarking"/><category term="lm"/><summary type="html"><![CDATA[Observing, Measuring the diversity of CodeLM Generations.]]></summary></entry></feed>