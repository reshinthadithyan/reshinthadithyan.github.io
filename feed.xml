<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://reshinthadithyan.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://reshinthadithyan.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-04-06T21:13:34+00:00</updated><id>https://reshinthadithyan.github.io/feed.xml</id><title type="html">Reshinth Adithyan</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Attempts to measure diversity of CodeLM Generations</title><link href="https://reshinthadithyan.github.io/blog/2023/code-diversity/" rel="alternate" type="text/html" title="Attempts to measure diversity of CodeLM Generations"/><published>2023-03-04T17:00:00+00:00</published><updated>2023-03-04T17:00:00+00:00</updated><id>https://reshinthadithyan.github.io/blog/2023/code-diversity</id><content type="html" xml:base="https://reshinthadithyan.github.io/blog/2023/code-diversity/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>Code Language Models are used in the context of code completion and chat interfaces day in day out by software developers to write and contribute to code bases. Programming Languages is a formal in nature with a fixed, strict set of rules. This pushes us to study quanitifying and measuring how diverse are the generations generated by these models in various settings, more importantly it makes us wonder what diversity is in the context of code. This post aims to explore and understand the diversity of generations of Code LMs. And discover various shortcomings in measuring diversity of generations of Code LMs in a controlled settings. In the below post I use <a href="https://github.com/openai/human-eval">HumanEval benchmark</a> which is a benchmark evaluating function level code completion by CodeLMs for functional correctness measuring the accuracy, passing the given test cases within first <code class="language-plaintext highlighter-rouge">k</code> out of <code class="language-plaintext highlighter-rouge">n</code> samples. This post widely tries to explore how to evaluate diversity in the context of code and shortcomings of the current methods to measure diversity.</p> <h2 id="semantic-and-syntactic-diversity">Semantic and Syntactic Diversity</h2> <p>The general notion of syntax and semantics in the context of programming languages is slightly different from that of natural language. In the context of programming languages syntax refers to the structure of the code, the rules that govern the structure of the code. Semantics refers to the intention of a given piece of code. Here is a small example of semantically similar code snippets which can be expressed varied syntax which are functionally equivalent.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/semantic_similar-480.webp 480w,/assets/img/semantic_similar-800.webp 800w,/assets/img/semantic_similar-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/semantic_similar.png" class="semantically similar code" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>The formal structure of the code allows for a wide range of syntactic variations. T Diversity of a CodeLM in the context of Code can be thus formally defined as <code class="language-plaintext highlighter-rouge">functionally equivalent, semantically similar</code> generations. While the idea of diversity might be very small in small snippets of code, more abstract real world software</p> <h2 id="humaneval-benchmark">HumanEval Benchmark</h2> <p><a href="https://github.com/openai/human-eval">HumanEval benchmark</a> introduced in the <a href="https://arxiv.org/abs/2107.03374">Codex</a> is a popular benchmark widely used to measure the performance of CodeLMs. It is a benchmark evaluating function level code completion by CodeLMs for functional correctness measuring the accuracy, passing the given test cases within first <code class="language-plaintext highlighter-rouge">k</code> out of <code class="language-plaintext highlighter-rouge">n</code> samples.</p> <h2 id="diversity-metrics">Diversity Metrics</h2> <p>The coherent way to quantify diversity would be to look at the latent representations of different completions given a prompt. This can be done by using a good embedding model and doing some cluster analysis in the latent representation of different completion given. While this method has itâ€™s shortcomings, it is a good starting point to quantify diversity. But the ideal end goal would be to measure how semantically diverse and functionally equivalent the generations are given a singular prompt.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/div_pipeline-480.webp 480w,/assets/img/div_pipeline-800.webp 800w,/assets/img/div_pipeline-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/div_pipeline.png" class="diversity codeLMs" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="limitation-of-code-embedding-models-to-model-semantics">Limitation of Code Embedding models to model semantics</h2> <p>Unlike natural language wherein, the semantics of a sentence can be understood by looking at the words and their order, in the context of programming languages, the semantics of a code snippet is not just the syntactical elements and their order. The semantics of a code snippet is also dependent on the structure of the code, the states used, the functions called, the libraries imported, etc. This makes it difficult to model the semantics of a code snippet using just the lexical tokens and their order. And this makes the model heavily biased to syntax rather than actual semantic similarity. For instance, consider three functions,</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">func_a</span><span class="p">(</span><span class="n">l</span><span class="p">):</span>
    <span class="k">return</span> <span class="nf">max</span><span class="p">(</span><span class="n">l</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">func_b</span><span class="p">(</span><span class="n">l</span><span class="p">):</span>
  <span class="n">max_element</span> <span class="o">=</span> <span class="n">l</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">l</span><span class="p">:</span>
      <span class="k">if</span> <span class="n">i</span> <span class="o">&gt;</span> <span class="n">max_element</span><span class="p">:</span>
          <span class="n">max_element</span> <span class="o">=</span> <span class="n">i</span>
  <span class="k">return</span> <span class="n">max_element</span>

<span class="k">def</span> <span class="nf">func_c</span><span class="p">(</span><span class="n">l</span><span class="p">):</span>
  <span class="n">max_element</span> <span class="o">=</span> <span class="n">l</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> 
  <span class="k">return</span> <span class="n">max_element</span>
</code></pre></div></div> <p><code class="language-plaintext highlighter-rouge">func_a</code> and <code class="language-plaintext highlighter-rouge">func_b</code> are semantically similar functionally trying to find the maximum element in a list. But <code class="language-plaintext highlighter-rouge">func_c</code> is actually bugged though naming conventions of the variable tends to be make the model map the latents to be similar. Though the actual semantically similar functional equivalent of <code class="language-plaintext highlighter-rouge">func_a</code> is <code class="language-plaintext highlighter-rouge">func_b</code>. The model tend to be severely biased towards the syntax rather than the actual semantics of the code.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/embd_plot-480.webp 480w,/assets/img/embd_plot-800.webp 800w,/assets/img/embd_plot-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/embd_plot.png" class="Embedding dataset" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="observations">Observations</h2> <p>For the sake of controlled experiment, we take the following models and do the analysis on the following models,</p> <ul> <li>deepseek-coder-1.3B</li> <li>deepseek-coder-6.7B</li> <li>deepseek-coder-33B</li> </ul> <p>We use <code class="language-plaintext highlighter-rouge">n_samples=10</code> and <code class="language-plaintext highlighter-rouge">temperature=0.2</code>to maintain consistency across scale. <br/> The choice is with the assumption that they are trained on similar if not same amount of data and are base models with no high quality fine-tuning data encapsulated in the training run, since the dynamics of generation quality might differ post finetuning phase. We use average pairwise distance and squared sum error as the metrics to measure the closeness and tighness of the latent representations of the generations.</p> <div> <iframe width="711" height="229" seamless="" frameborder="0" scrolling="no" src="https://docs.google.com/spreadsheets/d/e/2PACX-1vS52RmWqYdfJTxe_gfQScU3eGNIdGpZ-Zs2LqQ6rMq4tOVBiPmONN6Dn-BiC7ks3Sam6B4B7Y-XN9DP/pubchart?oid=598483660&amp;format=interactive"></iframe> </div> <p>First grasp of the results tends to conclude that the generations of the smaller models are more diverse compared to that of larger scales, but reality is quite different.</p> <h3 id="scenario-1---easy-to-solve-problem">Scenario-1 - Easy to solve problem</h3> <p>These are the problems that are mostly solved by all three model in <code class="language-plaintext highlighter-rouge">pass@100/pass@10</code> cap. A careful exploration on the generation reveals how the 1.3B model specifically generate code which has different newlines count and white space count. An easy-to-solve example of the same is shown below. In the case the 1.3B model the code generated was not functionally correct in 6 out of 10 cases.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#1.3B--example-1
</span><span class="kn">from</span> <span class="n">typing</span> <span class="kn">import</span> <span class="n">List</span> 
<span class="k">def</span> <span class="nf">has_close_elements</span><span class="p">(</span><span class="n">numbers</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">],</span><span class="n">threshold</span><span class="p">:</span> <span class="nb">float</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span> 
    <span class="n">numbers</span><span class="p">.</span><span class="nf">sort</span><span class="p">()</span> 
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nf">len</span><span class="p">(</span><span class="n">numbers</span><span class="p">)):</span> 
      <span class="k">if</span> <span class="n">numbers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="n">numbers</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">&lt;</span> <span class="n">threshold</span><span class="p">:</span> 
        <span class="k">return</span> <span class="bp">True</span> 
    <span class="k">return</span> <span class="bp">False</span> 
<span class="c1">#1.3B--example-2
</span><span class="k">def</span> <span class="nf">has_close_elements</span><span class="p">(</span><span class="n">numbers</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">],</span><span class="n">threshold</span><span class="p">:</span> <span class="nb">float</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span> 
    <span class="n">numbers</span><span class="p">.</span><span class="nf">sort</span><span class="p">()</span> 
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">1</span> <span class="p">,</span> <span class="nf">len</span><span class="p">(</span><span class="n">numbers</span><span class="p">))</span> <span class="p">:</span> 
      <span class="k">if</span> <span class="n">numbers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="n">numbers</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">&lt;</span> <span class="n">threshold</span> <span class="p">:</span> 
        <span class="k">return</span> <span class="bp">True</span> 
    <span class="k">return</span> <span class="bp">False</span> 
</code></pre></div></div> <p>In case of the same example generated by 6.7B and 33B model, the model consistantly the exact same code snippet for easy problems.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">typing</span> <span class="kn">import</span> <span class="n">List</span> 
<span class="k">def</span> <span class="nf">has_close_elements</span><span class="p">(</span><span class="n">numbers</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="n">threshold</span><span class="p">:</span> <span class="nb">float</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span> 
  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">numbers</span><span class="p">)):</span> 
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="nf">len</span><span class="p">(</span><span class="n">numbers</span><span class="p">)):</span> 
      <span class="k">if</span> <span class="nf">abs</span><span class="p">(</span><span class="n">numbers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="n">numbers</span><span class="p">[</span><span class="n">j</span><span class="p">])</span> <span class="o">&lt;</span> <span class="n">threshold</span><span class="p">:</span> 
        <span class="k">return</span> <span class="bp">True</span> 
  <span class="k">return</span> <span class="bp">False</span> 
</code></pre></div></div> <h3 id="exact-ast-match--performance">Exact AST match &amp; Performance</h3> <table> <thead> <tr> <th style="text-align: left">model name</th> <th style="text-align: center">AST exact match</th> <th style="text-align: right">pass@1</th> </tr> </thead> <tbody> <tr> <td style="text-align: left">Deepseek-coder-1.3B</td> <td style="text-align: center">60.7%</td> <td style="text-align: right">30.2%</td> </tr> <tr> <td style="text-align: left">Deepseek-coder-6.7B</td> <td style="text-align: center">45.6%</td> <td style="text-align: right">45.0%</td> </tr> <tr> <td style="text-align: left">Deepseek-coder-33B</td> <td style="text-align: center">31.7%</td> <td style="text-align: right">53.6%</td> </tr> </tbody> </table> <p>The above table was populated to measure exact AST matches of generations which are parseable valid code. The pass@1 metric is the percentage of generations that pass the test cases within the first generation. The results are quite interesting, the 1.3B model has the highest exact AST match amidst the generations. This is inline with the hypothesis that diversity is directly correlated with performance. More diverse the generations, more likely the generations contain the solution. Reiterating the diversity is defined as <code class="language-plaintext highlighter-rouge">functionally equivalent, semantically similar</code> generations.</p> <p>You can explore through the solutions generated by these models <a href="https://huggingface.co/spaces/reshinthadith/CodeGen-Diversity">here</a>.</p> <h2 id="conclusion-and-future-directions">Conclusion and Future Directions</h2> <p>Proper measure of diversity defined as <code class="language-plaintext highlighter-rouge">functionally equivalent, semantically similar</code> can act as a secondary metric to count onto in addition to performance of a model in correctness of generating solution for the given problem. When a model generates high quality diverse generations, that increases the probability of the output search space containing the solution of solving a problem in a given turn. The current methods to measure diversity in the code are heavily biased towards the syntax rather than the actual semantics of the code which represents the actual useful diversity. The future directions would be to explore more on how to measure the actual diversity of the generations of Code LMs.</p> <h2 id="cite">Cite</h2> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@inproceedings{reshint-code-lm-div,
    title = "How diverse are the generations of CodeLMs ?",
    author = "Reshinth Adithyan",
    month = april,
    year = "2024"}
</code></pre></div></div>]]></content><author><name></name></author><category term="codelms"/><category term="analysis"/><category term="benchmarking"/><category term="lm"/><summary type="html"><![CDATA[Observing, Measuring the diversity of CodeLM Generations.]]></summary></entry></feed>